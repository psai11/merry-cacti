# -*- coding: utf-8 -*-
"""Merry-Cacti.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hzgr3NXKEJDy0w2P7XXxRl3SzZWENVJo
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install wordninja
!pip install scattertext && python -m spacy.en.download
!pip install sklearn

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

pd.set_option('display.max_columns', 100)
sns.set_style("darkgrid")

from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image

import wordninja

# %matplotlib inline
import scattertext as st
import re, io
from pprint import pprint
from scipy.stats import rankdata, hmean, norm
import spacy
import os, pkgutil, json, urllib
from urllib.request import urlopen
from IPython.display import IFrame
from IPython.core.display import display, HTML
from scattertext import CorpusFromPandas, produce_scattertext_explorer
display(HTML("<style>.container { width:98% !important; }</style>"))

from google.colab import drive
drive.mount('/content/drive')

model_data = pd.read_csv('/content/drive/MyDrive/Merry Cacti/data/combined_data.csv', keep_default_na=False)

model_data.head(3)

model_data.info()

model_data['is_suicide'].mean()
#baseline accuracy of 60.25%

def multi_modelling(columns_list, model):
    for i in columns_list:
        X = model_data[i]
        y = model_data['is_suicide']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
        
        cvec = CountVectorizer()
        cvec.fit(X_train)
        
        X_train = pd.DataFrame(cvec.transform(X_train).todense(),
                               columns=cvec.get_feature_names())
        X_test = pd.DataFrame(cvec.transform(X_test).todense(),
                               columns=cvec.get_feature_names())
        
        nb = MultinomialNB()
        nb.fit(X_train,y_train)
        
        pred = nb.predict(X_test)
        
        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
        
        
        nb.predict_proba(X_test)
        pred_proba = [i[1] for i in nb.predict_proba(X_test)] 
        auc = roc_auc_score(y_test, pred_proba)

        classi_dict = (classification_report(y_test,pred, output_dict=True))

        model_results = {}
        model_results['series used (X)'] = i
        model_results['model'] = model
        model_results['AUC Score'] = auc
        model_results['precision']= classi_dict['weighted avg']['precision']
        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']
        model_results['confusion matrix']={"TP": tp,"FP":fp, "TN": tn, "FN": fn}
        model_results['train accuracy'] = nb.score(X_train, y_train)
        model_results['test accuracy'] = nb.score(X_test, y_test)
        model_results['baseline accuracy']=0.5166
        model_results['specificity']= tn/(tn+fp)  
        model_results['f1-score']= classi_dict['weighted avg']['f1-score']

        model_results
        df_list.append(model_results) 

    pd.set_option("display.max_colwidth", 50)
    return (pd.DataFrame(df_list)).round(2)

columns_list = ['title', 'selftext']
model = "CountVec + MultinomialNB"
df_list=[]
multi_modelling(columns_list, model)

X = model_data["title"]
y = model_data['is_suicide']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
X_test_tvec = tvec_optimised.transform(X_test).todense()

nb = MultinomialNB()
nb.fit(X_train_tvec, y_train)
accuracy = nb.score(X_test_tvec, y_test)

pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] 
auc = roc_auc_score(y_test, pred_proba)

print("ACCURACY: {}\nAUC SCORE: {}".format(accuracy, auc))

X = model_data["selftext"]
y = model_data['is_suicide']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
X_test_tvec = tvec_optimised.transform(X_test).todense()

nb = MultinomialNB()
nb.fit(X_train_tvec, y_train)
accuracy = nb.score(X_test_tvec, y_test)

pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] 
auc = roc_auc_score(y_test, pred_proba)

print("ACCURACY: {}\nAUC SCORE: {}".format(accuracy, auc) )

model_data2 = pd.read_csv('/content/drive/MyDrive/Merry Cacti/data/testdata.csv', keep_default_na=False)

model_data2['is_suicide'].mean()
#baseline accuracy of 65.18%

X = model_data2["title"]
y = model_data2['is_suicide']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
X_test_tvec = tvec_optimised.transform(X_test).todense()

nb = MultinomialNB()
nb.fit(X_train_tvec, y_train)
accuracy = nb.score(X_test_tvec, y_test)

pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] 
auc = roc_auc_score(y_test, pred_proba)

print("ACCURACY: {}\nAUC SCORE: {}".format(accuracy, auc))

X = model_data2["selftext"]
y = model_data2['is_suicide']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
X_test_tvec = tvec_optimised.transform(X_test).todense()

nb = MultinomialNB()
nb.fit(X_train_tvec, y_train)
accuracy = nb.score(X_test_tvec, y_test)

pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] 
auc = roc_auc_score(y_test, pred_proba)

print("ACCURACY: {}\nAUC SCORE: {}".format(accuracy, auc) )

pip install praw

import praw
import os
import sys
from praw.exceptions import APIException
import random as r
import time as t
import csv

userAgent = ''
cID = ''
cSC= ''
userN = ''
userP =''
reddit = praw.Reddit(user_agent=userAgent, client_id=cID, client_secret=cSC, username=userN, password=userP)

subreddit = reddit.subreddit('rant+SuicideWatch+depression+AmItheAsshole+SadThoughts')
keywords = ['pineapples']
bot_phrase = ['ily', 'you are loved', 'your life matters', 'it will be better soon', 'you deserve the world', 'you deserve to be happy', 'you are such a huge inspiration to many of us', 'you are beautiful inside and out', 'never lose hope', 'somewhere out there, someone cares about you', 'you are worthy', "i may not understand what you're going through, but i believe in you", 'the sun will still rise']
duplicateComments = []
duplicateSubmissions = []

def output():
  item_url = 'https://www.reddit.com' + submission.permalink
  print("URL: ", item_url)
  print("BODY: ", submission.selftext)
  if type_to == 'title':
    print("TITLE: ", submission.title)
  elif type_to == 'body':
    print("BODY2: ", submission.selftext)
  elif type_to == 'comment':
    print("COMMENT: ", comment.body)

type_to = 'body'
b_confirm = False

type_to = 'title'
t_confirm = False

for submission in subreddit.new(limit=10):
  n_body = submission.selftext.lower()
  if n_body == "":
    n_body = " "
  n_title = submission.title.lower()
  author = str(submission.author)
  s_id = submission.id
  s_url = 'https://www.reddit.com' + submission.permalink

  with open('reddit.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["title", "selftext", "url"])
    writer.writerow([n_title, n_body, s_url])
    file.close()
    redditdata = pd.read_csv('../content/reddit.csv', keep_default_na=False)

  if s_id in duplicateSubmissions:
    print("duplicate")
  else:
    duplicateSubmissions.append(s_id)
    print('Current post: https://www.reddit.com' + submission.permalink)
    X_train = model_data["title"]
    y_train = model_data['is_suicide']
    X_test = redditdata["title"]
    
    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
    X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
    X_test_tvec = tvec_optimised.transform(X_test).todense()
    nb = MultinomialNB()
    nb.fit(X_train_tvec, y_train)
    predictions_t = nb.predict(X_test_tvec)

    X_train = model_data["selftext"]
    y_train = model_data['is_suicide']
    X_test = redditdata["selftext"]

    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')
    X_train_tvec = tvec_optimised.fit_transform(X_train).todense()
    X_test_tvec = tvec_optimised.transform(X_test).todense()
    nb = MultinomialNB()
    nb.fit(X_train_tvec, y_train)
    predictions_b = nb.predict(X_test_tvec)

      # RELEVANT CODE STARTS HERE - if either Body or Title returned 1 from the model, reply with a random phrase
    if predictions_t == 1 or predictions_b == 1:
      t.sleep(5)
      try:
        message = r.choice(bot_phrase)
        submission.reply(message)
        output()

        #print("TARG: ",type(str(author)))
        #reddit.redditor(author).message("TEST", "Testing.")
      except APIException as e:
        print("\nComment not sent due to exceeding Reddit ratelimit.\n----------------------\n")
    else:
      print("Not Depressed.", duplicateSubmissions)

"""comments?"""